{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uncertainty': 3.0,\n",
       " 'negation': 10.5,\n",
       " 'time': 7.5,\n",
       " 'space': 5.0,\n",
       " 'vocabulary': 1.0,\n",
       " 'modality': 6.0,\n",
       " 'theory of mind': 10.5,\n",
       " 'reasoning': 8.5,\n",
       " 'composition': 3.0,\n",
       " 'anaphora': 8.0,\n",
       " 'noise': 3.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = [\"mmlu_computer_security\",\"mmlu_us_foreign_policy\",\"mmlu_college_chemistry\",\"lsat_qa\",\"legal_support\",\n",
    "         \"openbookqa\",\"bbq\",\"hellaswag\",\"truthfulqa\",\"mmlu_econometrics\",\"abstract_narrative_understanding\",\n",
    "         \"epistemic_reasoning\",\"formal_fallacies_syllogisms_negation\"]\n",
    "bigbench = [\"abstract_narrative_understanding\",\"epistemic_reasoning\",\"formal_fallacies_syllogisms_negation\"]\n",
    "\n",
    "bettermeta = [\"epistemic_reasoning\", \"bbq\", \"formal_fallacies_syllogisms_negation\", \"legal_support\", \"hellaswag\", \n",
    "              \"truthfulqa\", \"mmlu_college_chemistry\", \"abstract_narrative_understanding\"]\n",
    "\n",
    "bettermetrics = [\"mmlu_computer_security\",\"mmlu_us_foreign_policy\",\"mmlu_econometrics\"]\n",
    "\n",
    "same = [\"openbookqa\",\"lsat_qa\"]\n",
    "\n",
    "\n",
    "\n",
    "metafeatures = {'uncertainty' :0, 'negation':0, 'time':0, 'space':0,'vocabulary':0,'modality':0, \n",
    "             'theory of mind':0,'reasoning':0, 'composition':0,'anaphora':0, 'noise':0}\n",
    "# metafeatures = {'Scrabble' :0, 'TTR':0, 'FORCAST.RGL':0, 'K':0,'Flesch':0,'FOG':0,'SMOG':0}\n",
    "\n",
    "tasks = same #CHANGE BY THE CATEGORY OF TASKS YOU WANT TO ANALYZE\n",
    "\n",
    "for task in tasks:\n",
    "    if task in bigbench:\n",
    "        benchmark = \"bigbench\"\n",
    "    else:\n",
    "        benchmark = \"helm\"\n",
    "    \n",
    "    df = pd.read_csv(f'./{benchmark}/{task}/rf_noIRT_difficulty_feature_importance.csv')\n",
    "    df=df.rename(columns = {'Unnamed: 0':'feature'})\n",
    "    for index,row in df.iterrows():\n",
    "        metafeatures[row[\"feature\"]]+= index+1\n",
    "\n",
    "for feature in metafeatures:\n",
    "    metafeatures[feature] /= len(tasks)\n",
    "    \n",
    "metafeatures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
